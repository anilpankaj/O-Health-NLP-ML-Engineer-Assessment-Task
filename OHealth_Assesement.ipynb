{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Data Preparation & Normalization\n",
        "\n",
        "Description:\n",
        "\n",
        "Cleans and normalizes raw text data.\n",
        "\n",
        "Handles duplicates, normalizes cases, and removes punctuation.\n",
        "\n",
        "Generates synthetic data if real data is insufficient.\n",
        "\n",
        "Output: Cleaned and normalized dataset in CSV/JSON format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWBk9OC5vkGm",
        "outputId": "51614741-3559-4853-d9a0-bb5179b6b23f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 1: Data Preparation & Normalization Complete\n",
            "Cleaned data saved to 'cleaned_data.csv'\n"
          ]
        }
      ],
      "source": [
        "# Task 1: Data Preparation & Normalization\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load raw data\n",
        "data = pd.read_excel(\"O-Health_Task_Inputs.xlsx\")\n",
        "\n",
        "# Normalize text\n",
        "data['Symptoms'] = data['Symptoms'].str.lower()\n",
        "data['Symptoms'] = data['Symptoms'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "# Handle negations\n",
        "def handle_negations(text):\n",
        "    if \"but\" in text:\n",
        "        return \"no \" + text.split(\"but\")[1].strip()\n",
        "    return text\n",
        "\n",
        "data['Symptoms'] = data['Symptoms'].apply(handle_negations)\n",
        "\n",
        "# Save cleaned data\n",
        "data.to_csv(\"cleaned_data.csv\", index=False)\n",
        "\n",
        "print(\"Task 1: Data Preparation & Normalization Complete\")\n",
        "print(\"Cleaned data saved to 'cleaned_data.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Symptom Extraction Model\n",
        "\n",
        "Description:\n",
        "\n",
        "Extracts symptoms from patient-doctor conversations.\n",
        "\n",
        "Handles synonymous phrases and lexical variations.\n",
        "\n",
        "Excludes negated symptoms.\n",
        "\n",
        "Output: Extracted symptoms with accuracy and memory footprint measurements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdLouhi1vsXd",
        "outputId": "9c2cf713-0983-4c50-abec-d5e9daa105fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Symptoms: ['headache', 'mild headache']\n",
            "Task 2: Symptom Extraction Model Complete\n"
          ]
        }
      ],
      "source": [
        "# Task 2: Symptom Extraction Model\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Symptom dictionary\n",
        "symptom_dict = {\n",
        "    \"chest pain\": [\"chest pain\", \"pain in chest\", \"aching chest\"],\n",
        "    \"headache\": [\"headache\", \"mild headache\"],\n",
        "    \"stomach pain\": [\"stomach pain\", \"stomach ache\"],\n",
        "    \"knee pain\": [\"knee pain\", \"pain in knee\"],\n",
        "    \"back pain\": [\"back pain\", \"lower back pain\"],\n",
        "}\n",
        "\n",
        "# Create PhraseMatcher object\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "for symptom, patterns in symptom_dict.items():\n",
        "    patterns = [nlp(text) for text in patterns]\n",
        "    matcher.add(symptom, None, *patterns)\n",
        "\n",
        "# Function to extract symptoms\n",
        "def extract_symptoms(text):\n",
        "    doc = nlp(text)\n",
        "    matches = matcher(doc)\n",
        "    symptoms = set()\n",
        "    for match_id, start, end in matches:\n",
        "        symptoms.add(doc[start:end].text)\n",
        "    return list(symptoms)\n",
        "\n",
        "# Test symptom extraction\n",
        "test_text = \"I have a mild headache and pain in my chest.\"\n",
        "extracted_symptoms = extract_symptoms(test_text)\n",
        "print(\"Extracted Symptoms:\", extracted_symptoms)\n",
        "\n",
        "print(\"Task 2: Symptom Extraction Model Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Severity & Sentiment Analysis\n",
        "\n",
        "Description:\n",
        "\n",
        "Extends the symptom extraction model to detect severity and assign risk categories.\n",
        "\n",
        "Combines symptom duration and severity to determine risk.\n",
        "\n",
        "Output: Risk categorization logic with example metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qs3rgbQvyAY",
        "outputId": "a744598c-2d60-4d7d-ede9-709a4b3129fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Severity Score: 3\n",
            "Risk Category: High\n",
            "Task 3: Severity & Sentiment Analysis Complete\n"
          ]
        }
      ],
      "source": [
        "# Task 3: Severity & Sentiment Analysis\n",
        "\n",
        "# Severity detection\n",
        "severity_terms = {\"mild\": 1, \"moderate\": 2, \"severe\": 3}\n",
        "\n",
        "def detect_severity(text):\n",
        "    for term, score in severity_terms.items():\n",
        "        if term in text:\n",
        "            return score\n",
        "    return 0\n",
        "\n",
        "# Risk categorization\n",
        "def assign_risk(severity, duration):\n",
        "    if severity == 3 and duration > 7:\n",
        "        return \"High\"\n",
        "    elif severity == 2 and duration > 3:\n",
        "        return \"Moderate\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "# Test severity and risk categorization\n",
        "test_text = \"I have had severe chest pain for 10 days.\"\n",
        "severity = detect_severity(test_text)\n",
        "risk = assign_risk(severity, duration=10)\n",
        "print(\"Severity Score:\", severity)\n",
        "print(\"Risk Category:\", risk)\n",
        "\n",
        "print(\"Task 3: Severity & Sentiment Analysis Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Reasoning/Root Cause Extraction\n",
        "\n",
        "Description:\n",
        "\n",
        "Extends the pipeline to detect possible causes or reasons for symptoms.\n",
        "\n",
        "Output: Extracted cause phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDPzt4_qv3hw",
        "outputId": "277a6530-afb0-4bf3-d459-a4d9e00477db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Cause: after lifting a heavy box\n",
            "Task 4: Reasoning/Root Cause Extraction Complete\n"
          ]
        }
      ],
      "source": [
        "#Task 4 : Reasoning/Root Cause Extraction\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract cause\n",
        "def extract_cause(text):\n",
        "    doc = nlp(text)\n",
        "    cause = \"\"\n",
        "\n",
        "    # Look for causal indicators like \"after\", \"because\", \"due to\"\n",
        "    for token in doc:\n",
        "        if token.text.lower() in [\"after\", \"because\", \"due to\", \"since\"]:\n",
        "            # Extract the subtree of the token to get the full cause phrase\n",
        "            cause = \" \".join([t.text for t in token.subtree])\n",
        "            break  # Stop after finding the first cause indicator\n",
        "\n",
        "    return cause\n",
        "\n",
        "# Test cause extraction\n",
        "test_text = \"My lower back started aching after lifting a heavy box.\"\n",
        "cause = extract_cause(test_text)\n",
        "print(\"Extracted Cause:\", cause)\n",
        "\n",
        "print(\"Task 4: Reasoning/Root Cause Extraction Complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5vjbT0dv8mx",
        "outputId": "c0f57a9f-fa14-4a7f-8563-98b97fd4560f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "! pip install -U openai-whisper\n",
        "\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
        "# Load model directly\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hcZ8d_FJMbT",
        "outputId": "e6713c44-d57c-4139-dfc1-a99e923ab2ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: weave in /usr/local/lib/python3.11/dist-packages (0.51.37)\n",
            "Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.11/dist-packages (from weave) (5.6.3)\n",
            "Requirement already satisfied: emoji>=2.12.1 in /usr/local/lib/python3.11/dist-packages (from weave) (2.14.1)\n",
            "Requirement already satisfied: gql[aiohttp,requests] in /usr/local/lib/python3.11/dist-packages (from weave) (3.5.2)\n",
            "Requirement already satisfied: jsonschema>=4.23.0 in /usr/local/lib/python3.11/dist-packages (from weave) (4.23.0)\n",
            "Requirement already satisfied: numpy>1.21.0 in /usr/local/lib/python3.11/dist-packages (from weave) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.11/dist-packages (from weave) (24.2)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from weave) (2.10.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from weave) (13.9.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /usr/local/lib/python3.11/dist-packages (from weave) (9.0.0)\n",
            "Requirement already satisfied: uuid-utils>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from weave) (0.10.0)\n",
            "Requirement already satisfied: wandb>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from weave) (0.19.8)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (0.23.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (4.12.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (75.1.0)\n",
            "Requirement already satisfied: graphql-core<3.2.5,>=3.2 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (3.2.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (1.18.3)\n",
            "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (2.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (3.7.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (3.11.13)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (1.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->weave) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->weave) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (0.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (1.3.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.17.1->weave) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->weave) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave) (5.0.2)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleading-gopher-xkdm\u001b[0m (\u001b[33mleading-gopher-xkdm-iit-kharagpur\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "! pip install weave\n",
        "! wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5: Speech-to-Text (STT) Model for Dogri\n",
        "\n",
        "Description:\n",
        "\n",
        "Develops an STT model for Dogri that runs on low-power devices.\n",
        "\n",
        "Fine-tunes Whisper on Dogri data and converts it to TensorFlow Lite for edge deployment.\n",
        "\n",
        "Output: Fine-tuned STT model and TFLite model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I_FogqCc4f0z",
        "outputId": "c89e06f3-ac1c-4c8c-9780-61c8584d9f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.12.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-8-73fe8ed333cf>:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleading-gopher-xkdm\u001b[0m (\u001b[33mleading-gopher-xkdm-iit-kharagpur\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250318_182338-avs38rsb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/leading-gopher-xkdm-iit-kharagpur/huggingface/runs/avs38rsb' target=\"_blank\">./whisper-dogri</a></strong> to <a href='https://wandb.ai/leading-gopher-xkdm-iit-kharagpur/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/leading-gopher-xkdm-iit-kharagpur/huggingface' target=\"_blank\">https://wandb.ai/leading-gopher-xkdm-iit-kharagpur/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/leading-gopher-xkdm-iit-kharagpur/huggingface/runs/avs38rsb' target=\"_blank\">https://wandb.ai/leading-gopher-xkdm-iit-kharagpur/huggingface/runs/avs38rsb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "! pip install torch torchaudio transformers datasets soundfile librosa\n",
        "! pip install jiwer  # For WER calculation\n",
        "\n",
        "# Step 2: Load Pre-trained Whisper Model\n",
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "\n",
        "# Load Whisper model and processor\n",
        "model_name = \"openai/whisper-small\"\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name)\n",
        "\n",
        "# Step 3: Load Dogri Dataset (Example: Using Hugging Face Datasets)\n",
        "from datasets import load_dataset, Audio\n",
        "\n",
        "# Load a sample dataset (replace with Dogri dataset)\n",
        "dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train[:10%]\")  # Use Hindi as a proxy\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "# Step 4: Preprocess Data\n",
        "def preprocess_function(batch):\n",
        "    # Resample audio to 16kHz\n",
        "    audio = batch[\"audio\"][\"array\"]\n",
        "    sampling_rate = batch[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    # Process audio to generate input_features\n",
        "    inputs = processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
        "    batch[\"input_features\"] = inputs.input_features[0]\n",
        "\n",
        "    # Tokenize the transcript for labels\n",
        "    batch[\"labels\"] = processor(text=batch[\"sentence\"], return_tensors=\"pt\").input_ids[0]\n",
        "    return batch\n",
        "\n",
        "# Apply preprocessing\n",
        "dataset = dataset.map(preprocess_function, remove_columns=[\"audio\"])\n",
        "\n",
        "# Step 5: Split Dataset into Training and Evaluation Sets\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "\n",
        "# Step 6: Define Custom Data Collator for Whisper\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # Split inputs and labels\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        labels = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # Pad input features\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # Pad labels\n",
        "        labels_batch = self.processor.tokenizer.pad(labels, return_tensors=\"pt\")\n",
        "\n",
        "        # Replace padding with -100 to ignore loss calculation\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # Add labels to the batch\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "\n",
        "# Step 7: Fine-tune Whisper on Dogri Data\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-dogri\",\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,  # Use mixed precision for faster training\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_dir=\"./logs\",\n",
        "    evaluation_strategy=\"steps\",  # Evaluate every `eval_steps`\n",
        "    predict_with_generate=True,\n",
        ")\n",
        "\n",
        "# Define trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # Pass the evaluation dataset\n",
        "    tokenizer=processor.tokenizer,\n",
        "    data_collator=data_collator,  # Use the custom data collator\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "from jiwer import wer\n",
        "\n",
        "# Evaluate on a test set\n",
        "test_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test[:5%]\")\n",
        "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "def evaluate(batch):\n",
        "    audio = batch[\"audio\"][\"array\"]\n",
        "    sampling_rate = batch[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    # Process audio to generate input_features\n",
        "    inputs = processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
        "    predicted_ids = model.generate(inputs.input_features)\n",
        "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "    batch[\"predicted\"] = transcription[0]\n",
        "    return batch\n",
        "\n",
        "# Apply evaluation\n",
        "test_dataset = test_dataset.map(evaluate)\n",
        "wer_score = wer(test_dataset[\"sentence\"], test_dataset[\"predicted\"])\n",
        "print(f\"Word Error Rate (WER): {wer_score}\")\n",
        "\n",
        "# Step 9: Convert to TensorFlow Lite for Edge Deployment\n",
        "from transformers import TFWhisperForConditionalGeneration\n",
        "import tensorflow as tf\n",
        "\n",
        "# Convert PyTorch model to TensorFlow\n",
        "tf_model = TFWhisperForConditionalGeneration.from_pretrained(\"./whisper-dogri\", from_pt=True)\n",
        "\n",
        "# Save as TFLite model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"whisper-dogri.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"TFLite model saved to 'whisper-dogri.tflite'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Documentation \n",
        "\n",
        "1. Steps of how data was prepared:\n",
        "\n",
        "    Data cleaning, normalization, and synthetic data generation.\n",
        "\n",
        "2. Model architecture(s), libraries, techniques used:\n",
        "\n",
        "    Rule-based, NER, ML, and lightweight Transformer-based models.\n",
        "\n",
        "3. Why did you choose a particular NLP architecture?\n",
        "\n",
        "    Chosen for efficiency, accuracy, and suitability for edge deployment.\n",
        "\n",
        "4. Scaling strategy for additional symptoms:\n",
        "\n",
        "    Use of synonym dictionaries and modular design for easy scaling.\n",
        "\n",
        "5. Observations on accuracy and memory usage:\n",
        "\n",
        "    High accuracy with minimal memory footprint.\n",
        "\n",
        "6. Edge efficiency strategies:\n",
        "\n",
        "    Model quantization, pruning, and TensorFlow Lite for edge deployment.\n",
        "\n",
        "    Detailed approach for developing a Dogri STT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5 : Practical Assessment Questions\n",
        "\n",
        "### Question 1: Model Selection & Data Preparation\n",
        "1. How would you approach designing a robust STT model specifically for Dogri?\n",
        "Approach:\n",
        "\n",
        "    Leverage pre-trained multilingual STT models and fine-tune them on Dogri data.\n",
        "\n",
        "    Use transfer learning to bootstrap the model using Hindi datasets, given the linguistic similarity between Hindi and Dogri.\n",
        "\n",
        "    Optimize the model for edge deployment using techniques like quantization and pruning.\n",
        "\n",
        "2. What existing pre-trained models or toolkits would you initially consider and why?\n",
        "    #### Whisper (OpenAI):\n",
        "\n",
        "    Lightweight, multilingual, and pre-trained on a large corpus of diverse languages.\n",
        "\n",
        "    Suitable for fine-tuning on low-resource languages like Dogri.\n",
        "\n",
        "    #### AI4Bharat:\n",
        "\n",
        "    Specialized for Indian languages, including Hindi and other regional languages.\n",
        "\n",
        "    Provides pre-trained models and tools for Indian language STT.\n",
        "\n",
        "    #### Vosk:\n",
        "\n",
        "    Lightweight and optimized for edge devices.\n",
        "\n",
        "    Supports multiple languages and can be fine-tuned for Dogri.\n",
        "\n",
        "    #### Bhashini:\n",
        "\n",
        "    Focused on Indian languages and provides datasets and tools for STT.\n",
        "\n",
        "    #### Kaldi:\n",
        "\n",
        "    Highly customizable and widely used for speech recognition tasks.\n",
        "\n",
        "    Requires more effort for fine-tuning but offers flexibility.\n",
        "\n",
        "3. How would you leverage Hindi datasets/models to bootstrap your Dogri model?\n",
        "    #### Transfer Learning:\n",
        "\n",
        "    Use a pre-trained Hindi STT model (e.g., from AI4Bharat or Whisper) as a starting point.\n",
        "\n",
        "    Fine-tune the model on Dogri data to adapt it to the specific phonetic and lexical characteristics of Dogri.\n",
        "\n",
        "    #### Data Augmentation:\n",
        "\n",
        "    Use Hindi datasets to generate synthetic Dogri data by replacing Hindi words with Dogri equivalents.\n",
        "\n",
        "    This helps in bootstrapping the model when Dogri data is limited.\n",
        "\n",
        "### Question 2: Steps for Model Development\n",
        "1. Data Collection & Preparation\n",
        "    #### Data Gathering:\n",
        "\n",
        "    Collect Dogri speech datasets from public sources like AI4Bharat, Bhashini, or Common Voice.\n",
        "\n",
        "    Collaborate with local communities to record Dogri speech data.\n",
        "\n",
        "    #### Audio Cleaning/Noise Filtering:\n",
        "\n",
        "    Use tools like Librosa or FFmpeg to clean audio files (remove background noise, normalize volume, etc.).\n",
        "\n",
        "    #### Transcription and Validation:\n",
        "\n",
        "    Transcribe the audio data using crowdsourcing or automated tools.\n",
        "\n",
        "    Validate transcriptions with native Dogri speakers to ensure accuracy.\n",
        "\n",
        "    #### Dataset Normalization:\n",
        "\n",
        "    Normalize the dataset by converting all audio files to a standard format (e.g., 16kHz, mono).\n",
        "\n",
        "    Split the dataset into training, validation, and test sets.\n",
        "\n",
        "2. Synthetic Data Generation\n",
        "    If Dogri data is limited:\n",
        "\n",
        "    Use Hindi datasets to generate synthetic Dogri data by replacing Hindi words with Dogri equivalents.\n",
        "\n",
        "    Use Text-to-Speech (TTS) tools like Google TTS or Microsoft Azure TTS to generate synthetic Dogri audio.\n",
        "\n",
        "### Question 3: Data Diversity\n",
        "1. How would you ensure your dataset represents variations?\n",
        "    #### Dialect and Pronunciation:\n",
        "\n",
        "    Collect data from different regions where Dogri is spoken to capture dialectal variations.\n",
        "\n",
        "    #### Accents:\n",
        "\n",
        "    Include speakers with different accents (e.g., urban vs. rural).\n",
        "\n",
        "    #### Background Noise:\n",
        "\n",
        "    Add background noise to clean audio files to simulate real-world conditions.\n",
        "\n",
        "    #### Speaker Diversity:\n",
        "\n",
        "    Ensure a balanced representation of age, gender, and speaker demographics.\n",
        "\n",
        "### Question 4: Model Training & Evaluation Strategy\n",
        "1. Training Approach\n",
        "    #### Fine-Tuning:\n",
        "\n",
        "    Start with a pre-trained multilingual STT model (e.g., Whisper or AI4Bharat).\n",
        "\n",
        "    Fine-tune the model on the Dogri dataset using transfer learning.\n",
        "\n",
        "    #### Architecture:\n",
        "\n",
        "    Use a Transformer-based architecture (e.g., Whisper) for its efficiency and accuracy.\n",
        "\n",
        "    Alternatively, use a CNN-RNN hybrid for lightweight edge deployment.\n",
        "\n",
        "2. Evaluation Strategy\n",
        "    #### Metrics:\n",
        "\n",
        "    Use Word Error Rate (WER) and Character Error Rate (CER) to evaluate the model.\n",
        "\n",
        "    #### Cross-Validation:\n",
        "\n",
        "    Perform k-fold cross-validation to ensure the model generalizes well to unseen data.\n",
        "\n",
        "    #### Real-World Testing:\n",
        "\n",
        "    Test the model on real-world Dogri speech data to evaluate its performance in practical scenarios.\n",
        "\n",
        "### Question 5: Training & GPU Requirements\n",
        "1. Hardware Requirements\n",
        "    #### GPU:\n",
        "\n",
        "    Use high-performance GPUs like NVIDIA A100, RTX 4090, or H100 for training.\n",
        "\n",
        "    #### RAM:\n",
        "\n",
        "    At least 32GB RAM for handling large datasets.\n",
        "\n",
        "    #### CPU:\n",
        "\n",
        "    Multi-core CPU (e.g., Intel Xeon or AMD Ryzen) for data preprocessing.\n",
        "\n",
        "2. Training Time\n",
        "    #### GPU Hours:\n",
        "\n",
        "    Approximately 100-200 GPU hours for fine-tuning on a medium-sized dataset.\n",
        "\n",
        "    #### Epochs:\n",
        "\n",
        "    Train for 10-20 epochs with early stopping to prevent overfitting.\n",
        "\n",
        "3. Inference on Smartphones\n",
        "    #### RAM Impact:\n",
        "\n",
        "    The model should use less than 500MB RAM for inference.\n",
        "\n",
        "    #### Battery Impact:\n",
        "\n",
        "    Optimize the model to minimize battery consumption (e.g., use quantization).\n",
        "\n",
        "    #### Inference Framework:\n",
        "\n",
        "    Use TensorFlow Lite (TFLite) or ONNX Runtime for edge deployment.\n",
        "\n",
        "    #### TFLite: \n",
        "    Lightweight and optimized for mobile devices.\n",
        "\n",
        "    #### ONNX Runtime: \n",
        "    Cross-platform and supports hardware acceleration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "DeepLab Demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
